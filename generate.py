import torch
import logging
from model import GPTLanguageModel
from config import *
from tokenizer import build_vocab, encode, decode

# =============================
# üì¶ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# =============================
logging.basicConfig(level=logging.INFO, filename="generation.log", filemode="a", format="%(asctime)s [%(levelname)s] %(message)s")

# =============================
# üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–ª–æ–≤–∞—Ä—è
# =============================

with open(data_path, 'r', encoding='utf-8') as f:
    text = f.read()

stoi, itos = build_vocab(text)
vocab_size = len(stoi)

# =======================
# üßê –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
# =======================

def load_model():
    model = GPTLanguageModel(vocab_size)
    path = "models/best_model.pth" if load_best_model else "models/model.pth"
    model.load_state_dict(torch.load(path, map_location=device))
    model.to(device)
    model.eval()
    logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å {path}")
    return model

# =======================
# ‚ú® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
# =======================

def generate_text(prompt: str = '', max_new_tokens=200, temperature=1.0, top_k=10, on_token=None) -> str:
    if not prompt.strip():
        return "‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç"

    if not (0.1 <= temperature <= 5.0):
        return "‚õîÔ∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ—Ç 0.1 –¥–æ 5.0"
    if not (1 <= top_k <= vocab_size):
        return f"‚õîÔ∏è top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ {vocab_size}"

    model = load_model()
    encoded_prompt = encode(prompt, stoi)

    unknown_count = len(prompt) - len(encoded_prompt)
    if unknown_count > 0:
        logging.warning(f"‚ö†Ô∏è {unknown_count} –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã")

    context = torch.tensor([encoded_prompt], dtype=torch.long).to(device)

    if context.size(1) > block_size:
        logging.warning(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç —É—Å–µ–∫–∞–µ—Ç—Å—è –¥–æ {block_size} —Ç–æ–∫–µ–Ω–æ–≤")
        context = context[:, -block_size:]

    logging.info(f"üîÆ –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: prompt='{prompt}'")
    generated = context

    with torch.no_grad():
        for _ in range(max_new_tokens):
            idx_cond = generated[:, -block_size:]
            logits, _ = model(idx_cond)
            logits = logits[:, -1, :] / temperature

            if top_k is not None:
                values, indices = torch.topk(logits, top_k)
                logits = torch.full_like(logits, float('-inf'))
                logits.scatter_(1, indices, values)

            probs = torch.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            generated = torch.cat((generated, next_token), dim=1)

            next_char = itos.get(next_token.item(), '')
            if on_token:
                on_token(next_char)
            else:
                print(next_char, end='', flush=True)

            if next_char == '\n':
                break

    logging.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    decoded = decode(generated[0].tolist(), itos).strip()
    if not decoded:
        return "‚ö†Ô∏è –ù–∏—á–µ–≥–æ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å."
    return decoded

# =======================
# ü•≥ –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
# =======================
def generate_batch(prompts: list[str], **params) -> list[str]:
    return [generate_text(prompt=p, **params) for p in prompts]
