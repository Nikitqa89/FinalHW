"""
–ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π GPT-—è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.

–§—É–Ω–∫—Ü–∏—è generate_text(prompt) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
–≤–≤–µ–¥—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
"""

import os
import torch
import re
from transformer_model.model import GPTLanguageModel
from config import *

# ============================
# üì• –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# ============================

base_dir = os.path.dirname(__file__)
data_prepared_path = os.path.join(base_dir, "data_prepared.pt")
model_checkpoint_path = os.path.join(base_dir, "best_model.pt" if load_best_model else "model.pt")

train_data, val_data, stoi, itos = torch.load(data_prepared_path)
vocab_size = len(stoi)

model = GPTLanguageModel(vocab_size)
model.load_state_dict(torch.load(model_checkpoint_path, map_location=device))
model.eval().to(device)

# ========================
# üî† –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞
# ========================

def encode(s: str) -> list[int]:
    encoded = [stoi[c] for c in s if c in stoi]
    if not encoded:
        raise ValueError("‚õî –í —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
    return encoded

def decode(indices: list[int]) -> str:
    return ''.join([itos[i] for i in indices])

# ========================
# üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å—é
# ========================

@torch.no_grad()
def generate_text(prompt: str = "", max_new_tokens: int = 100, temperature: float = 1.0, top_k: int = 10) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏.
    """
    idx = torch.tensor([encode(prompt)], dtype=torch.long).to(device)
    eos_token = stoi.get('<EOS>')

    for _ in range(max_new_tokens):
        idx_cond = idx[:, -block_size:]
        logits, _ = model(idx_cond)
        logits = logits[:, -1, :] / temperature
        probs = torch.softmax(logits, dim=-1)

        if top_k is not None:
            top_probs, top_idx = torch.topk(probs, k=top_k, dim=-1)
            top_probs = top_probs / torch.sum(top_probs, dim=-1, keepdim=True)
            next_token = top_idx.gather(-1, torch.multinomial(top_probs, num_samples=1))
        else:
            next_token = torch.multinomial(probs, num_samples=1)

        if eos_token is not None and next_token.item() == eos_token:
            break

        idx = torch.cat((idx, next_token), dim=1)

    raw_text = decode(idx[0].tolist())

    # –ó–∞–º–µ–Ω—è–µ–º <EOS> –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    final_text = raw_text.replace('<EOS>', '\n\n')

    # –ß–∏—Å—Ç–∏–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
    lines = [line.strip() for line in final_text.split('\n')]
    formatted_lines = [line for line in lines if len(line) > 1]
    clean_text = '\n'.join(formatted_lines)
    clean_text = re.sub(r'\n{3,}', '\n\n', clean_text).strip()

    return f"{clean_text}"