"""
GPTLanguageModel ‚Äî —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π GPT-–º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è.

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- Multi-Head Self Attention
- Position-wise Feed Forward
- Layer Normalization
- Residual Connections
- Embedding —Å–ª–æ–∏
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
from config import *

# ============================================
# üéØ –û–¥–∏–Ω self-attention "–≥–æ–ª–æ–≤–∞" (–≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞)
# ============================================

class Head(nn.Module):
    """
    –û–¥–Ω–∞ "–≥–æ–ª–æ–≤–∞" —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è (Self-Attention).

    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥ –≤ –∫–ª—é—á–∏, –∑–∞–ø—Ä–æ—Å—ã –∏ –∑–Ω–∞—á–µ–Ω–∏—è,
    –∑–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è —Å –º–∞—Å–∫–æ–π –¥–ª—è –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç–∏.
    """
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)

        # –ù–∏–∂–Ω–µ—Ç—Ä–µ—É–≥–æ–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç–∏ (future tokens –∑–∞–ø—Ä–µ—â–µ–Ω—ã)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)

        # Attention weights
        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, T)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        wei = self.dropout(wei)

        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
        v = self.value(x)
        return wei @ v  # (B, T, C)

# ===================================================
# üîÅ –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (–Ω–µ—Å–∫–æ–ª—å–∫–æ Head –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å)
# ===================================================

class MultiHeadAttention(nn.Module):
    """
    –ù–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ).

    –ò—Ö –≤—ã—Ö–æ–¥—ã –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –∏ –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ.
    """
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        return self.dropout(self.proj(out))

# ========================================
# üß† FeedForward ‚Äî MLP –º–µ–∂–¥—É attention –±–ª–æ–∫–∞–º–∏
# ========================================

class FeedForward(nn.Module):
    """
    –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å GELU-–∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π.

    –†–∞—Å—à–∏—Ä—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤ 4 —Ä–∞–∑–∞, –∑–∞—Ç–µ–º –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ.
    """
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.GELU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)

# ======================
# üîÅ –ë–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
# ======================

class Block(nn.Module):
    """
    –û–¥–∏–Ω –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞:
    Attention + FeedForward + Residual + LayerNorm
    """
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        # –°–Ω–∞—á–∞–ª–∞ attention —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ —Ä–µ–∑–∏–¥—É–∞–ª—å–Ω—ã–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º
        x = x + self.sa(self.ln1(x))
        # –ü–æ—Ç–æ–º FeedForward —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ —Ä–µ–∑–∏–¥—É–∞–ª—å–Ω—ã–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º
        x = x + self.ffwd(self.ln2(x))
        return x

# =======================================
# üß† GPT-–º–æ–¥–µ–ª—å –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–ª–æ–∫–æ–≤
# =======================================

class GPTLanguageModel(nn.Module):
    """
    –ü–æ–ª–Ω–∞—è GPT-–º–æ–¥–µ–ª—å:
    - –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π
    - –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–ª–æ–∫–æ–≤ (Attention + FeedForward)
    - –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
    """
    def __init__(self, vocab_size):
        super().__init__()

        # –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)

        # –ë–ª–æ–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd)  # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

        # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
        self.lm_head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –º–æ–¥–µ–ª–∏.

        Args:
            idx: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –∏–Ω–¥–µ–∫—Å–æ–≤ (B, T)
            targets: –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (B, T), –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø–æ—Ç–µ—Ä—å

        Returns:
            logits: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (B, T, vocab_size)
            loss: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω—ã targets)
        """
        B, T = idx.shape

        tok_emb = self.token_embedding_table(idx)  # (B, T, C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)
        x = tok_emb + pos_emb  # –°—É–º–º–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π

        x = self.blocks(x)  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –±–ª–æ–∫–∏
        x = self.ln_f(x)    # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        logits = self.lm_head(x)  # (B, T, vocab_size)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        loss = None
        if targets is not None:
            B, T, C = logits.shape
            logits = logits.view(B * T, C)
            targets = targets.view(B * T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss