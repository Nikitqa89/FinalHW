import os
import time
import torch
import matplotlib.pyplot as plt
from model import GPTLanguageModel
from config import *

# ================================
# üìÅ –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
# ================================

base_dir = os.path.dirname(__file__)
data_prepared_path = os.path.join(base_dir, "data_prepared.pt")
final_model_path = os.path.join(base_dir, "model.pt")
loss_plot_path = os.path.join(base_dir, "loss_plot.png")
best_model_path = os.path.join(base_dir, "best_model.pt")

# ================================
# üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# ================================

train_data, val_data, stoi, itos = torch.load(data_prepared_path)
vocab_size = len(stoi)
print(f"üß† –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}, –¢–æ–∫–µ–Ω–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏: {len(train_data)}")

# ======================
# üì¶ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞
# ======================

def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i + block_size] for i in ix])
    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])
    return x.to(device), y.to(device)

# ================================
# üìâ –†–∞—Å—á—ë—Ç –ø–æ—Ç–µ—Ä—å –Ω–∞ train/val
# ================================

@torch.no_grad()
def estimate_loss(model):
    model.eval()
    losses = {'train': [], 'val': []}
    for split in ['train', 'val']:
        total = 0
        for _ in range(eval_iters):
            xb, yb = get_batch(split)
            _, loss = model(xb, yb)
            total += loss.item()
        avg = total / eval_iters
        losses[split].append(avg)
    model.train()
    return {k: sum(v) / len(v) for k, v in losses.items()}

# =======================
# üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# =======================

model = GPTLanguageModel(vocab_size).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

# üîÅ –î–æ–±–∞–≤–∏–º scheduler —Å warmup
warmup_steps = 500
scheduler = torch.optim.lr_scheduler.LambdaLR(
    optimizer,
    lambda step: min((step + 1) / warmup_steps, 1.0)
)

log_steps, train_losses, val_losses = [], [], []
best_val_loss = float('inf')

print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...\n")
start_time = time.time()

for step in range(max_iters):
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ª–æ–≥
    if step % eval_interval == 0:
        losses = estimate_loss(model)
        elapsed = time.time() - start_time
        mins, secs = divmod(int(elapsed), 60)
        print(f"Step {step}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f} ‚Äî ‚è± {mins}m {secs}s")
        log_steps.append(step)
        train_losses.append(losses['train'])
        val_losses.append(losses['val'])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å
        ckpt_path = os.path.join(base_dir, f"model_step_{step}.pt")
        torch.save(model.state_dict(), ckpt_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if losses['val'] < best_val_loss:
            best_val_loss = losses['val']
            torch.save(model.state_dict(), best_model_path)
            print(f"üíæ ‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞ (val loss = {best_val_loss:.4f})")

    # –û–±—É—á–µ–Ω–∏–µ
    xb, yb = get_batch('train')
    logits, loss = model(xb, yb)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    scheduler.step()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
torch.save(model.state_dict(), final_model_path)
print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_model_path}")

# –í—Ä–µ–º—è
total_time = time.time() - start_time
mins, secs = divmod(int(total_time), 60)
print(f"\nüèÅ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {mins} –º–∏–Ω {secs} —Å–µ–∫")

# =======================
# üìà –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
# =======================

plt.plot(log_steps, train_losses, label='Train loss')
plt.plot(log_steps, val_losses, label='Val loss')
plt.xlabel("–ò—Ç–µ—Ä–∞—Ü–∏–∏")
plt.ylabel("Loss")
plt.legend()
plt.title("–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏")
plt.grid(True)
plt.tight_layout()
plt.savefig(loss_plot_path)
plt.show()